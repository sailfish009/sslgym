{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "EkoNwGki0u9G",
    "2_h_M4b_k4Gn",
    "BlztjllM_Qg8",
    "SNL0T1rS_kPv",
    "pLKd3GbR_pnK",
    "w-vpjPyB_waV",
    "d8-4sI4Jnj3c",
    "vogoYGtQk6GP",
    "ljylHL0n96sQ",
    "3aIw2zdi99iM",
    "fGS1EUmH2q2y",
    "s7i29dKd2ts_",
    "gwRL15xS3HwH",
    "wVtRtJ8Y7YDm"
   ],
   "toc_visible": true,
   "private_outputs": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/SauravMaheshkar/Self-Supervised-Learning/blob/main/notebooks/DINO/PyTorch_DINO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Author:** [@SauravMaheshkar](https://twitter.com/MaheshkarSaurav)"
   ],
   "metadata": {
    "id": "zNttV2LO0nbp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* No Mixed Precision\n",
    "* No Gradient Clipping\n",
    "* Doesn't the keep the output layer fixed"
   ],
   "metadata": {
    "id": "Vd1jCRRT4XOO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üì¶  Packages and Basic Setup\n",
    "---"
   ],
   "metadata": {
    "id": "EkoNwGki0u9G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install -U rich\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from typing import Sequence, List, Iterable\n",
    "from PIL import ImageFilter, ImageOps\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from rich import print\n",
    "from rich.progress import track\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models as torchvision_models"
   ],
   "metadata": {
    "id": "b2lKNAcNsUji"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title ‚öô Configuration\n",
    "saveckp_freq = 2  # @param {type: \"number\"}\n",
    "random_seed = 42  # @param {type: \"number\"}\n",
    "\n",
    "\n",
    "# ============ Random Seed ... ==========\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(seed=random_seed)"
   ],
   "metadata": {
    "id": "hrUqU4LCWjHs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üÜò Utility Classes and Functions\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "2_h_M4b_k4Gn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üññ Utilites for Data Augmentation"
   ],
   "metadata": {
    "id": "BlztjllM_Qg8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"\n",
    "    Apply Gaussian Blur to the PIL image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, p: float = 0.5, radius_min: float = 0.1, radius_max: float = 2.0\n",
    "    ) -> None:\n",
    "        self.prob = p\n",
    "        self.radius_min = radius_min\n",
    "        self.radius_max = radius_max\n",
    "\n",
    "    def __call__(self, img):\n",
    "        do_it = random.random() <= self.prob\n",
    "        if not do_it:\n",
    "            return img\n",
    "\n",
    "        return img.filter(\n",
    "            ImageFilter.GaussianBlur(\n",
    "                radius=random.uniform(self.radius_min, self.radius_max)\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    \"\"\"\n",
    "    Apply Solarization to the PIL image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p: float) -> None:\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img"
   ],
   "metadata": {
    "id": "CWVv-1sVsMYl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚úÇÔ∏è Multi-Crop Strategy"
   ],
   "metadata": {
    "id": "SNL0T1rS_kPv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "in the multi-crop strategy from a given image (say $x$) a set is generated consisting of $V$ different views of the image i.e. $\\large \\{ x_i \\}_{i=1}^{V}$. This set contains two \"**global**\" (standard resolution crops) say $\\large x_1^g$ and $\\large x_2^g$, and several other \"**local**\" (low resolution crops) views. Using low resolution crops  ensures only a small increase in compute cost.All the local crops are passed through the student whereas only the global views are passed through the teacher i.e. encouraging \"**local-to-global**\" correspondences. The following loss is minimized.\n",
    "\n",
    "$$\n",
    "\\huge \\displaystyle\\min_{\\theta_s} \\hspace{1em} \\sum_{x \\in \\{ x_1^g, x_2^g \\}} \\hspace{0.5em} \\sum_{\\begin{array}{}x' \\in V \\\\ x' \\neq x\\end{array}} \\hspace{0.75em} H(\\, P_t(x), P_s(x')\\,)\n",
    "$$"
   ],
   "metadata": {
    "id": "2D9Qs4420v2T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform forward pass separately on each resolution input.\n",
    "    The inputs corresponding to a single resolution are clubbed and single\n",
    "    forward is run on the same resolution inputs. Hence we do several\n",
    "    forward passes = number of different resolutions used. We then\n",
    "    concatenate all the output features and run the head forward on these\n",
    "    concatenated features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone: nn.Module, head: nn.Module) -> None:\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        # disable layers dedicated to ImageNet labels classification\n",
    "        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x: List) -> torch.Tensor:\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            x = [x]\n",
    "        idx_crops = torch.cumsum(\n",
    "            torch.unique_consecutive(\n",
    "                torch.tensor([inp.shape[-1] for inp in x]),\n",
    "                return_counts=True,\n",
    "            )[1],\n",
    "            0,\n",
    "        )\n",
    "        start_idx, output = 0, torch.empty(0).to(x[0].device)\n",
    "        for end_idx in idx_crops:\n",
    "            _out = self.backbone(torch.cat(x[start_idx:end_idx]))\n",
    "            # The output is a tuple with XCiT model. See:\n",
    "            # https://github.com/facebookresearch/xcit/blob/master/xcit.py#L404-L405\n",
    "            if isinstance(_out, tuple):\n",
    "                _out = _out[0]\n",
    "            # accumulate outputs\n",
    "            output = torch.cat((output, _out))\n",
    "            start_idx = end_idx\n",
    "        # Run the head forward on the concatenated features.\n",
    "        return self.head(output)"
   ],
   "metadata": {
    "id": "__UcKRrSmxF1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üìâ The Loss Function"
   ],
   "metadata": {
    "id": "pLKd3GbR_pnK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "given an input image $x$ the teacher network produces a vector of scores $\\large s^t(x) = [\\,s^{t}_{1}(x),s^{t}_{2}(x), ..., s^{t}_{K}(x) \\,]$ which are then converted into (\"**soft**\") probabilities $\\large p^{t}_{k}(x) = \\frac{e^{s^{t}_{k}(x)}}{\\sum_j e^{s^{t}_{k}(x)}}$. These probabilities are usually softened using temperature scaling , and the loss that the student trains for is a linear combination of the cross-entropy loss $\\mathbb{L}_{cls}$ and a Knowledge Distillation Loss $\\mathbb{L}_{KD}$ viz.\n",
    "\n",
    "$$\n",
    "\\huge \\displaystyle \\mathbb{L}_{KD} = - \\tau^2 \\sum_{k} \\tilde{p}^{t}_{k} (x) + log \\tilde{p}^{s}_{k}(x)\n",
    "$$\n",
    "$$\n",
    "\\huge \\displaystyle \\mathbb{L} = \\alpha \\mathbb{L}_{cls} + (1-\\alpha) \\mathbb{L}_{KD}\n",
    "$$"
   ],
   "metadata": {
    "id": "JAsFD4_E2DSE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DINOLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_dim,\n",
    "        ncrops,\n",
    "        warmup_teacher_temp,\n",
    "        teacher_temp,\n",
    "        warmup_teacher_temp_epochs,\n",
    "        nepochs,\n",
    "        student_temp=0.1,\n",
    "        center_momentum=0.9,\n",
    "    ):\n",
    "        super(DINOLoss, self).__init__()\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.ncrops = ncrops\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "        # we apply a warm up for the teacher temperature because\n",
    "        # a too high temperature makes the training instable at the beginning\n",
    "        self.teacher_temp_schedule = np.concatenate(\n",
    "            (\n",
    "                np.linspace(\n",
    "                    warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs\n",
    "                ),\n",
    "                np.ones(nepochs - warmup_teacher_temp_epochs) * teacher_temp,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, student_output, teacher_output, epoch):\n",
    "        \"\"\"\n",
    "        Cross-entropy between softmax outputs of the teacher and student networks.\n",
    "        \"\"\"\n",
    "        student_out = student_output / self.student_temp\n",
    "        student_out = student_out.chunk(self.ncrops)\n",
    "\n",
    "        # teacher centering and sharpening\n",
    "        temp = self.teacher_temp_schedule[epoch]\n",
    "        teacher_out = F.softmax((teacher_output - self.center) / temp, dim=-1)\n",
    "        teacher_out = teacher_out.detach().chunk(2)\n",
    "\n",
    "        total_loss = 0\n",
    "        n_loss_terms = 0\n",
    "        for iq, q in enumerate(teacher_out):\n",
    "            for v in range(len(student_out)):\n",
    "                if v == iq:\n",
    "                    # we skip cases where student and teacher operate on the same view\n",
    "                    continue\n",
    "                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)\n",
    "                total_loss += loss.mean()\n",
    "                n_loss_terms += 1\n",
    "        total_loss /= n_loss_terms\n",
    "        self.update_center(teacher_output)\n",
    "        return total_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"\n",
    "        Update center used for teacher output.\n",
    "        \"\"\"\n",
    "        batch_center = torch.sum(teacher_output, dim=0, keepdim=True)\n",
    "        # dist.all_reduce(batch_center)\n",
    "        batch_center = batch_center / len(teacher_output)\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (\n",
    "            1 - self.center_momentum\n",
    "        )"
   ],
   "metadata": {
    "id": "TbAXBZNCk3QL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üêô Head"
   ],
   "metadata": {
    "id": "w-vpjPyB_waV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DINOHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        use_bn: bool = False,\n",
    "        norm_last_layer: bool = True,\n",
    "        nlayers: int = 3,\n",
    "        hidden_dim: int = 2048,\n",
    "        bottleneck_dim: int = 256,\n",
    "    ):\n",
    "        super(DINOHead, self).__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.last_layer = nn.utils.weight_norm(\n",
    "            nn.Linear(bottleneck_dim, out_dim, bias=False)\n",
    "        )\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "D7gV5EpRlCok"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚õëÔ∏è Utility Functions"
   ],
   "metadata": {
    "id": "d8-4sI4Jnj3c"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_params_groups(model: nn.Module) -> Iterable:\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{\"params\": regularized}, {\"params\": not_regularized, \"weight_decay\": 0.0}]\n",
    "\n",
    "\n",
    "def cosine_scheduler(\n",
    "    base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0\n",
    "):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = final_value + 0.5 * (base_value - final_value) * (\n",
    "        1 + np.cos(np.pi * iters / len(iters))\n",
    "    )\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule"
   ],
   "metadata": {
    "id": "KOxCD0HSnlz4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üíø The Dataset\n",
    "---\n",
    "\n",
    "For the purposes of this colab, we use the CIFAR10 dataset to train the model using DINO for Multi-Class Image Classification."
   ],
   "metadata": {
    "id": "vogoYGtQk6GP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## üññ Data Augmentation Pipeline\n",
    "\n",
    "The various pipelines for the patches (both global and local)"
   ],
   "metadata": {
    "id": "ljylHL0n96sQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class DataAugmentationDINO(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_crops_scale: Sequence[float],\n",
    "        local_crops_scale: Sequence[float],\n",
    "        local_crops_number: int,\n",
    "    ) -> None:\n",
    "        flip_and_color_jitter = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply(\n",
    "                    [\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1\n",
    "                        )\n",
    "                    ],\n",
    "                    p=0.8,\n",
    "                ),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "            ]\n",
    "        )\n",
    "        normalize = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # first global crop\n",
    "        self.global_transfo1 = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    224,\n",
    "                    scale=global_crops_scale,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                flip_and_color_jitter,\n",
    "                GaussianBlur(1.0),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "        # second global crop\n",
    "        self.global_transfo2 = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    224,\n",
    "                    scale=global_crops_scale,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                flip_and_color_jitter,\n",
    "                GaussianBlur(0.1),\n",
    "                Solarization(0.2),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "        # transformation for the local small crops\n",
    "        self.local_crops_number = local_crops_number\n",
    "        self.local_transfo = transforms.Compose(\n",
    "            [\n",
    "                transforms.RandomResizedCrop(\n",
    "                    96,\n",
    "                    scale=local_crops_scale,\n",
    "                    interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "                ),\n",
    "                flip_and_color_jitter,\n",
    "                GaussianBlur(p=0.5),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> Sequence[torch.Tensor]:\n",
    "        crops = []\n",
    "        crops.append(self.global_transfo1(image))\n",
    "        crops.append(self.global_transfo2(image))\n",
    "        for _ in range(self.local_crops_number):\n",
    "            crops.append(self.local_transfo(image))\n",
    "        return crops"
   ],
   "metadata": {
    "id": "HpIOdO8wkwmO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ‚öôÔ∏è Dataloader"
   ],
   "metadata": {
    "id": "3aIw2zdi99iM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "transform = DataAugmentationDINO(\n",
    "    global_crops_scale=(0.4, 1.0),\n",
    "    local_crops_scale=(0.05, 0.4),\n",
    "    local_crops_number=8,\n",
    ")\n",
    "dataset = datasets.CIFAR10(\"./data\", transform=transform, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "print(f\"Data loaded: there are {len(dataset)} images.\")"
   ],
   "metadata": {
    "id": "dvlfVP3GWnGG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ‚úçÔ∏è Model Architecture & Training\n",
    "---"
   ],
   "metadata": {
    "id": "fGS1EUmH2q2y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building Student and Teacher Networks\n"
   ],
   "metadata": {
    "id": "s7i29dKd2ts_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Instantiate Models\n",
    "student = torchvision_models.resnet18()\n",
    "teacher = torchvision_models.resnet18()\n",
    "embed_dim = student.fc.weight.shape[1]\n",
    "\n",
    "# multi-crop wrapper handles \"the forward method\" with inputs of different resolutions\n",
    "student = MultiCropWrapper(\n",
    "    student,\n",
    "    DINOHead(in_dim=embed_dim, out_dim=65536, use_bn=False, norm_last_layer=True),\n",
    ")\n",
    "teacher = MultiCropWrapper(\n",
    "    teacher,\n",
    "    DINOHead(in_dim=embed_dim, out_dim=65536, use_bn=False),\n",
    ")\n",
    "\n",
    "# move networks to GPU\n",
    "student, teacher = student.cuda(), teacher.cuda()\n",
    "teacher_without_ddp = teacher\n",
    "\n",
    "# teacher and student start with the same weights\n",
    "teacher_without_ddp.load_state_dict(student.state_dict())\n",
    "\n",
    "# Stop Gradient: there is no backpropagation through the teacher, so no need for gradients\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"Student and Teacher are built: they are both resnet18 network.\")"
   ],
   "metadata": {
    "id": "19gxovafW1y0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Loss, Optimizers and Schedulers"
   ],
   "metadata": {
    "id": "gwRL15xS3HwH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# The Loss Function\n",
    "dino_loss = DINOLoss(\n",
    "    65536,\n",
    "    8 + 2,  # total number of crops = 2 global crops + local_crops_number\n",
    "    0.04,\n",
    "    0.04,\n",
    "    0,\n",
    "    20,\n",
    ").cuda()\n",
    "\n",
    "# Optimizer\n",
    "params_groups = get_params_groups(student)\n",
    "optimizer = torch.optim.AdamW(params_groups)  # to use with ViTs\n",
    "\n",
    "# ============ init schedulers ... ============\n",
    "lr_schedule = cosine_scheduler(\n",
    "    0.0005 * 32 / 256.0,  # linear scaling rule\n",
    "    1e-6,\n",
    "    20,\n",
    "    len(data_loader),\n",
    "    warmup_epochs=10,\n",
    ")\n",
    "wd_schedule = cosine_scheduler(\n",
    "    0.04,\n",
    "    0.4,\n",
    "    20,\n",
    "    len(data_loader),\n",
    ")\n",
    "# momentum parameter is increased to 1. during training with a cosine schedule\n",
    "momentum_schedule = cosine_scheduler(0.996, 1, 20, len(data_loader))\n",
    "print(f\"Loss, optimizer and schedulers ready.\")"
   ],
   "metadata": {
    "id": "QaiRCYnaXA_p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training\n",
    "\n",
    "![](https://github.com/SauravMaheshkar/infographics/blob/main/DINO/DINO.png?raw=true)\n",
    "\n"
   ],
   "metadata": {
    "id": "wVtRtJ8Y7YDm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Starting DINO training !\")\n",
    "for epoch in range(0, 1):\n",
    "    for it, (images, _) in track(enumerate(data_loader), total=len(data_loader)):\n",
    "        # update weight decay and learning rate according to their schedule\n",
    "        it = len(data_loader) * epoch + it  # global training iteration\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:  # only the first group is regularized\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "\n",
    "        # move images to gpu\n",
    "        images = [im.cuda(non_blocking=True) for im in images]\n",
    "        # teacher and student forward passes + compute dino loss\n",
    "        teacher_output = teacher(\n",
    "            images[:2]\n",
    "        )  # only the 2 global views pass through the teacher\n",
    "        student_output = student(images)\n",
    "        loss = dino_loss(student_output, teacher_output, epoch)\n",
    "\n",
    "        if not math.isfinite(loss.item()):\n",
    "            print(\"Loss is {}, stopping training\".format(loss.item()), force=True)\n",
    "            sys.exit(1)\n",
    "\n",
    "        # student update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA update for the teacher\n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]  # momentum parameter\n",
    "            for param_q, param_k in zip(\n",
    "                student.parameters(), teacher_without_ddp.parameters()\n",
    "            ):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "\n",
    "        # logging\n",
    "        torch.cuda.synchronize()\n",
    "        # print(f\"loss:{loss.item()}\")\n",
    "        # print(f\"lr:{optimizer.param_groups[0]['lr']}\")\n",
    "        # print(f\"wd:{optimizer.param_groups[0]['weight_decay']}\")\n",
    "\n",
    "    # ============ writing logs ... ============\n",
    "    save_dict = {\n",
    "        \"student\": student.state_dict(),\n",
    "        \"teacher\": teacher.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"dino_loss\": dino_loss.state_dict(),\n",
    "    }\n",
    "    torch.save(save_dict, os.path.join(\"./\", \"checkpoint.pth\"))\n",
    "    if saveckp_freq and epoch % saveckp_freq == 0:\n",
    "        torch.save(save_dict, os.path.join(\"./\", f\"checkpoint{epoch:04}.pth\"))"
   ],
   "metadata": {
    "id": "dXspjA9Mamy0"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
